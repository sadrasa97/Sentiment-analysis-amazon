{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW0oSyx_lePi",
        "outputId": "607df50c-c953-4901-e5f1-56411e5b6fe8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "id": "hBf0LyOVSNZr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9111f510-3c3f-4446-e296-8546b0ce9564"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "RP9rcUGfQ46G"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = 'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Watches_v1_00.tsv.gz'\n",
        "\n",
        "# Load the TSV file from the URL into a pandas DataFrame\n",
        "df = pd.read_csv(url, delimiter='\\t', compression='gzip', error_bad_lines=False)\n",
        "\n",
        "# Display the first 5 rows of the DataFrame\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0_328pSzFCn",
        "outputId": "6ce56924-5c3d-4482-f545-0f4d3c6b839f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-1b351bc60c32>:6: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df = pd.read_csv(url, delimiter='\\t', compression='gzip', error_bad_lines=False)\n",
            "Skipping line 8704: expected 15 fields, saw 22\n",
            "Skipping line 16933: expected 15 fields, saw 22\n",
            "Skipping line 23726: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 85637: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 132136: expected 15 fields, saw 22\n",
            "Skipping line 158070: expected 15 fields, saw 22\n",
            "Skipping line 166007: expected 15 fields, saw 22\n",
            "Skipping line 171877: expected 15 fields, saw 22\n",
            "Skipping line 177756: expected 15 fields, saw 22\n",
            "Skipping line 181773: expected 15 fields, saw 22\n",
            "Skipping line 191085: expected 15 fields, saw 22\n",
            "Skipping line 196273: expected 15 fields, saw 22\n",
            "Skipping line 196331: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 197000: expected 15 fields, saw 22\n",
            "Skipping line 197011: expected 15 fields, saw 22\n",
            "Skipping line 197432: expected 15 fields, saw 22\n",
            "Skipping line 208016: expected 15 fields, saw 22\n",
            "Skipping line 214110: expected 15 fields, saw 22\n",
            "Skipping line 244328: expected 15 fields, saw 22\n",
            "Skipping line 248519: expected 15 fields, saw 22\n",
            "Skipping line 254936: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 272057: expected 15 fields, saw 22\n",
            "Skipping line 293214: expected 15 fields, saw 22\n",
            "Skipping line 310507: expected 15 fields, saw 22\n",
            "Skipping line 312306: expected 15 fields, saw 22\n",
            "Skipping line 316296: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 336028: expected 15 fields, saw 22\n",
            "Skipping line 344885: expected 15 fields, saw 22\n",
            "Skipping line 352551: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 408773: expected 15 fields, saw 22\n",
            "Skipping line 434535: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 581593: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 652409: expected 15 fields, saw 22\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
            "0          US      3653882  R3O9SGZBVQBV76  B00FALQ1ZC       937001370   \n",
            "1          US     14661224   RKH8BNC3L5DLF  B00D3RGO20       484010722   \n",
            "2          US     27324930  R2HLE8WKZSU3NL  B00DKYC7TK       361166390   \n",
            "3          US      7211452  R31U3UH5AZ42LL  B000EQS1JW       958035625   \n",
            "4          US     12733322  R2SV659OUJ945Y  B00A6GFD7S       765328221   \n",
            "\n",
            "                                       product_title product_category  \\\n",
            "0  Invicta Women's 15150 \"Angel\" 18k Yellow Gold ...          Watches   \n",
            "1  Kenneth Cole New York Women's KC4944 Automatic...          Watches   \n",
            "2  Ritche 22mm Black Stainless Steel Bracelet Wat...          Watches   \n",
            "3  Citizen Men's BM8180-03E Eco-Drive Stainless S...          Watches   \n",
            "4  Orient ER27009B Men's Symphony Automatic Stain...          Watches   \n",
            "\n",
            "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
            "0            5              0            0    N                 Y   \n",
            "1            5              0            0    N                 Y   \n",
            "2            2              1            1    N                 Y   \n",
            "3            5              0            0    N                 Y   \n",
            "4            4              0            0    N                 Y   \n",
            "\n",
            "                              review_headline  \\\n",
            "0                                  Five Stars   \n",
            "1  I love thiswatch it keeps time wonderfully   \n",
            "2                                   Two Stars   \n",
            "3                                  Five Stars   \n",
            "4    Beautiful face, but cheap sounding links   \n",
            "\n",
            "                                         review_body review_date  \n",
            "0  Absolutely love this watch! Get compliments al...  2015-08-31  \n",
            "1       I love this watch it keeps time wonderfully.  2015-08-31  \n",
            "2                                          Scratches  2015-08-31  \n",
            "3  It works well on me. However, I found cheaper ...  2015-08-31  \n",
            "4  Beautiful watch face.  The band looks nice all...  2015-08-31  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop irrelevant columns\n",
        "# Preprocessing\n",
        "df = df[['star_rating', 'review_body']]\n",
        "df.dropna(inplace=True)\n",
        "df['sentiment'] = df['star_rating'].apply(lambda rating: 'positive' if rating >= 4 else 'negative')\n",
        "\n"
      ],
      "metadata": {
        "id": "36x8zbaqwC8d"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into train, test, and validation sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['review_body'], df['sentiment'], test_size=0.2, random_state=42, stratify=df['sentiment'])\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42, stratify=y_train) # split 60-20-20\n",
        "\n",
        "# Feature extraction\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_counts = vectorizer.fit_transform(X_train)\n",
        "X_test_counts = vectorizer.transform(X_test)\n",
        "X_val_counts = vectorizer.transform(X_val)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ufn7MVtFwKv3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Naive Bayes classifier\n",
        "nb_clf = MultinomialNB()\n",
        "nb_clf.fit(X_train_counts, y_train)\n",
        "nb_pred_test = nb_clf.predict(X_test_counts)\n",
        "nb_pred_val = nb_clf.predict(X_val_counts)\n",
        "print(f\"Naive Bayes test accuracy: {accuracy_score(y_test, nb_pred_test):.2f}\")\n",
        "print(f\"Naive Bayes validation accuracy: {accuracy_score(y_val, nb_pred_val):.2f}\")\n",
        "print(\"Naive Bayes test classification report:\\n\", classification_report(y_test, nb_pred_test))\n",
        "print(\"Naive Bayes validation classification report:\\n\", classification_report(y_val, nb_pred_val))\n",
        "print(\"Naive Bayes test confusion matrix:\\n\", confusion_matrix(y_test, nb_pred_test))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAuTOdAAwO6k",
        "outputId": "fb7ec8c7-933f-4d00-bc6a-530d327219f1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes test accuracy: 0.88\n",
            "Naive Bayes validation accuracy: 0.88\n",
            "Naive Bayes test classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.75      0.68      0.71     43246\n",
            "    positive       0.91      0.93      0.92    148766\n",
            "\n",
            "    accuracy                           0.88    192012\n",
            "   macro avg       0.83      0.81      0.82    192012\n",
            "weighted avg       0.87      0.88      0.87    192012\n",
            "\n",
            "Naive Bayes validation classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.75      0.68      0.71     43245\n",
            "    positive       0.91      0.94      0.92    148766\n",
            "\n",
            "    accuracy                           0.88    192011\n",
            "   macro avg       0.83      0.81      0.82    192011\n",
            "weighted avg       0.87      0.88      0.87    192011\n",
            "\n",
            "Naive Bayes test confusion matrix:\n",
            " [[ 29348  13898]\n",
            " [  9685 139081]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Logistic Regression classifier\n",
        "lr_clf = LogisticRegression()\n",
        "lr_clf.fit(X_train_counts, y_train)\n",
        "lr_pred_test = lr_clf.predict(X_test_counts)\n",
        "lr_pred_val = lr_clf.predict(X_val_counts)\n",
        "print(f\"Logistic Regression test accuracy: {accuracy_score(y_test, lr_pred_test):.2f}\")\n",
        "print(f\"Logistic Regression validation accuracy: {accuracy_score(y_val, lr_pred_val):.2f}\")\n",
        "print(\"Logistic Regression test classification report:\\n\", classification_report(y_test, lr_pred_test))\n",
        "print(\"Logistic Regression validation classification report:\\n\", classification_report(y_val, lr_pred_val))\n",
        "print(\"Logistic Regression test confusion matrix:\\n\", confusion_matrix(y_test, lr_pred_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2i6FvgNwRr0",
        "outputId": "6e1aa61b-5829-4a5a-fbff-0aa64bee85cd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression test accuracy: 0.90\n",
            "Logistic Regression validation accuracy: 0.90\n",
            "Logistic Regression test classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.83      0.67      0.75     43246\n",
            "    positive       0.91      0.96      0.93    148766\n",
            "\n",
            "    accuracy                           0.90    192012\n",
            "   macro avg       0.87      0.82      0.84    192012\n",
            "weighted avg       0.89      0.90      0.89    192012\n",
            "\n",
            "Logistic Regression validation classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.83      0.67      0.74     43245\n",
            "    positive       0.91      0.96      0.93    148766\n",
            "\n",
            "    accuracy                           0.90    192011\n",
            "   macro avg       0.87      0.82      0.84    192011\n",
            "weighted avg       0.89      0.90      0.89    192011\n",
            "\n",
            "Logistic Regression test confusion matrix:\n",
            " [[ 29107  14139]\n",
            " [  5772 142994]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##part B"
      ],
      "metadata": {
        "id": "cHJIQSdQV5qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the dataset\n",
        "url = 'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Watches_v1_00.tsv.gz'\n",
        "\n",
        "# Load the TSV file from the URL into a pandas DataFrame\n",
        "df = pd.read_csv(url, delimiter='\\t', compression='gzip', error_bad_lines=False)\n",
        "\n",
        "# Preprocessing\n"
      ],
      "metadata": {
        "id": "s5k5H7U_V-V9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c679be1f-54d4-4214-ac01-66fa0e570df3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-4cb0e3eaa15d>:11: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df = pd.read_csv(url, delimiter='\\t', compression='gzip', error_bad_lines=False)\n",
            "Skipping line 8704: expected 15 fields, saw 22\n",
            "Skipping line 16933: expected 15 fields, saw 22\n",
            "Skipping line 23726: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 85637: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 132136: expected 15 fields, saw 22\n",
            "Skipping line 158070: expected 15 fields, saw 22\n",
            "Skipping line 166007: expected 15 fields, saw 22\n",
            "Skipping line 171877: expected 15 fields, saw 22\n",
            "Skipping line 177756: expected 15 fields, saw 22\n",
            "Skipping line 181773: expected 15 fields, saw 22\n",
            "Skipping line 191085: expected 15 fields, saw 22\n",
            "Skipping line 196273: expected 15 fields, saw 22\n",
            "Skipping line 196331: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 197000: expected 15 fields, saw 22\n",
            "Skipping line 197011: expected 15 fields, saw 22\n",
            "Skipping line 197432: expected 15 fields, saw 22\n",
            "Skipping line 208016: expected 15 fields, saw 22\n",
            "Skipping line 214110: expected 15 fields, saw 22\n",
            "Skipping line 244328: expected 15 fields, saw 22\n",
            "Skipping line 248519: expected 15 fields, saw 22\n",
            "Skipping line 254936: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 272057: expected 15 fields, saw 22\n",
            "Skipping line 293214: expected 15 fields, saw 22\n",
            "Skipping line 310507: expected 15 fields, saw 22\n",
            "Skipping line 312306: expected 15 fields, saw 22\n",
            "Skipping line 316296: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 336028: expected 15 fields, saw 22\n",
            "Skipping line 344885: expected 15 fields, saw 22\n",
            "Skipping line 352551: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 408773: expected 15 fields, saw 22\n",
            "Skipping line 434535: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 581593: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 652409: expected 15 fields, saw 22\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import gzip\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "url = 'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Watches_v1_00.tsv.gz'\n",
        "filename = 'amazon_reviews_us_Watches_v1_00.tsv.gz'\n",
        "\n",
        "# Download the compressed data from the URL\n",
        "response = requests.get(url, stream=True)\n",
        "\n",
        "# Save the compressed data to a file\n",
        "with open(filename, 'wb') as f:\n",
        "    shutil.copyfileobj(response.raw, f)\n",
        "\n",
        "# Extract the compressed data to a TSV file\n",
        "with gzip.open(filename, 'rb') as f_in:\n",
        "    with open('amazon_reviews_us_Watches_v1_00.tsv', 'wb') as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "# Load the TSV file into a pandas DataFrame\n",
        "df1 = pd.read_csv('amazon_reviews_us_Watches_v1_00.tsv', delimiter='\\t',on_bad_lines='skip')"
      ],
      "metadata": {
        "id": "gyZmqpwo2BN-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df1[['star_rating', 'review_body']]\n",
        "df1.dropna(inplace=True)\n",
        "df1['sentiment'] = df['star_rating'].apply(lambda rating: 'positive' if rating >= 4 else 'negative')\n",
        "\n"
      ],
      "metadata": {
        "id": "nhMIOvuu0vCs"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erkiPM0f46Bm",
        "outputId": "17e89a4b-8caa-467c-c5cf-2eeb813bd6f6"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 960056 entries, 0 to 960203\n",
            "Data columns (total 3 columns):\n",
            " #   Column       Non-Null Count   Dtype \n",
            "---  ------       --------------   ----- \n",
            " 0   star_rating  960056 non-null  int64 \n",
            " 1   review_body  960056 non-null  object\n",
            " 2   sentiment    960056 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 29.3+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into train, test, and validation sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df1['review_body'], df1['sentiment'], test_size=0.2, random_state=42, stratify=df1['sentiment'])\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42, stratify=y_train) # split 60-20-20\n",
        "\n"
      ],
      "metadata": {
        "id": "DEjnlLFA00Dh"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature extraction\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_counts = vectorizer.fit_transform(X_train)\n",
        "X_test_counts = vectorizer.transform(X_test)\n",
        "X_val_counts = vectorizer.transform(X_val)\n",
        "\n"
      ],
      "metadata": {
        "id": "PDvihxOx0309"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a neural network model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(16, activation='relu', input_shape=(X_train_counts.shape[-1],)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n"
      ],
      "metadata": {
        "id": "7Cj9e1AV08J9"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMRavzXi0_Wb",
        "outputId": "c54b48b5-7a04-49b3-b42d-81cf6040fbb7"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_13 (Dense)            (None, 16)                1689872   \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,689,889\n",
            "Trainable params: 1,689,889\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "# Download and extract the TSV file\n",
        "url = 'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Watches_v1_00.tsv.gz'\n",
        "filename = 'amazon_reviews_us_Watches_v1_00.tsv.gz'\n",
        "df = pd.read_csv(url, delimiter='\\t', compression='gzip', error_bad_lines=False)\n",
        "\n",
        "# Preprocess the data\n",
        "df = df[['star_rating', 'review_body', 'verified_purchase']]\n",
        "df = df.dropna()\n",
        "df = df[df['verified_purchase'] == 'Y']\n",
        "df['label'] = df['star_rating'].apply(lambda x: 1 if x > 3 else 0)\n",
        "df = df[['review_body', 'label']]\n",
        "df = df.sample(frac=1, random_state=42)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_size = int(0.8 * len(df))\n",
        "train_df = df[:train_size]\n",
        "valid_df = df[train_size:]\n",
        "\n",
        "# Vectorize the text data\n",
        "max_features = 10000\n",
        "sequence_length = 100\n",
        "vectorizer = TextVectorization(max_tokens=max_features, output_mode='int', output_sequence_length=sequence_length)\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(train_df['review_body'].values).batch(128)\n",
        "vectorizer.adapt(text_ds)\n",
        "\n",
        "# Create TensorFlow Datasets\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((vectorizer(train_df['review_body'].values), train_df['label'].values)).batch(32)\n",
        "valid_ds = tf.data.Dataset.from_tensor_slices((vectorizer(valid_df['review_body'].values), valid_df['label'].values)).batch(32)\n",
        "\n",
        "# Define your Keras model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(max_features + 1, 16),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_ds, epochs=10, validation_data=valid_ds)\n",
        "\n",
        "# Evaluate the model using the validation data\n",
        "valid_loss, valid_acc = model.evaluate(valid_ds)\n",
        "print(f'Validation accuracy: {valid_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEA_02Uj7iER",
        "outputId": "ceae4931-e9c4-46c0-db3a-d8dd21321a01"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-79-fb2f1f690696>:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df = pd.read_csv(url, delimiter='\\t', compression='gzip', error_bad_lines=False)\n",
            "Skipping line 8704: expected 15 fields, saw 22\n",
            "Skipping line 16933: expected 15 fields, saw 22\n",
            "Skipping line 23726: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 85637: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 132136: expected 15 fields, saw 22\n",
            "Skipping line 158070: expected 15 fields, saw 22\n",
            "Skipping line 166007: expected 15 fields, saw 22\n",
            "Skipping line 171877: expected 15 fields, saw 22\n",
            "Skipping line 177756: expected 15 fields, saw 22\n",
            "Skipping line 181773: expected 15 fields, saw 22\n",
            "Skipping line 191085: expected 15 fields, saw 22\n",
            "Skipping line 196273: expected 15 fields, saw 22\n",
            "Skipping line 196331: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 197000: expected 15 fields, saw 22\n",
            "Skipping line 197011: expected 15 fields, saw 22\n",
            "Skipping line 197432: expected 15 fields, saw 22\n",
            "Skipping line 208016: expected 15 fields, saw 22\n",
            "Skipping line 214110: expected 15 fields, saw 22\n",
            "Skipping line 244328: expected 15 fields, saw 22\n",
            "Skipping line 248519: expected 15 fields, saw 22\n",
            "Skipping line 254936: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 272057: expected 15 fields, saw 22\n",
            "Skipping line 293214: expected 15 fields, saw 22\n",
            "Skipping line 310507: expected 15 fields, saw 22\n",
            "Skipping line 312306: expected 15 fields, saw 22\n",
            "Skipping line 316296: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 336028: expected 15 fields, saw 22\n",
            "Skipping line 344885: expected 15 fields, saw 22\n",
            "Skipping line 352551: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 408773: expected 15 fields, saw 22\n",
            "Skipping line 434535: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 581593: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 652409: expected 15 fields, saw 22\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "20767/20767 [==============================] - 99s 5ms/step - loss: 0.3026 - accuracy: 0.8745 - val_loss: 0.2659 - val_accuracy: 0.8950\n",
            "Epoch 2/10\n",
            "20767/20767 [==============================] - 94s 5ms/step - loss: 0.2604 - accuracy: 0.8982 - val_loss: 0.2630 - val_accuracy: 0.8977\n",
            "Epoch 3/10\n",
            "20767/20767 [==============================] - 92s 4ms/step - loss: 0.2568 - accuracy: 0.9000 - val_loss: 0.2628 - val_accuracy: 0.8983\n",
            "Epoch 4/10\n",
            "20767/20767 [==============================] - 90s 4ms/step - loss: 0.2552 - accuracy: 0.9010 - val_loss: 0.2631 - val_accuracy: 0.8983\n",
            "Epoch 5/10\n",
            "20767/20767 [==============================] - 93s 5ms/step - loss: 0.2543 - accuracy: 0.9014 - val_loss: 0.2634 - val_accuracy: 0.8982\n",
            "Epoch 6/10\n",
            "20767/20767 [==============================] - 91s 4ms/step - loss: 0.2537 - accuracy: 0.9017 - val_loss: 0.2638 - val_accuracy: 0.8980\n",
            "Epoch 7/10\n",
            "20767/20767 [==============================] - 95s 5ms/step - loss: 0.2533 - accuracy: 0.9019 - val_loss: 0.2641 - val_accuracy: 0.8979\n",
            "Epoch 8/10\n",
            "20767/20767 [==============================] - 90s 4ms/step - loss: 0.2530 - accuracy: 0.9021 - val_loss: 0.2644 - val_accuracy: 0.8979\n",
            "Epoch 9/10\n",
            "20767/20767 [==============================] - 95s 5ms/step - loss: 0.2527 - accuracy: 0.9022 - val_loss: 0.2646 - val_accuracy: 0.8977\n",
            "Epoch 10/10\n",
            "20767/20767 [==============================] - 95s 5ms/step - loss: 0.2525 - accuracy: 0.9023 - val_loss: 0.2648 - val_accuracy: 0.8977\n",
            "5192/5192 [==============================] - 9s 2ms/step - loss: 0.2648 - accuracy: 0.8977\n",
            "Validation accuracy: 0.8977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2SOHv6jY05g",
        "outputId": "40c0663e-0045-4a01-b9d2-97c1e3358ada"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "# Download and extract the TSV file\n",
        "url = 'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Watches_v1_00.tsv.gz'\n",
        "filename = 'amazon_reviews_us_Watches_v1_00.tsv.gz'\n",
        "df = pd.read_csv(url, delimiter='\\t', compression='gzip', error_bad_lines=False)\n",
        "\n",
        "# Preprocess the data\n",
        "df = df[['star_rating', 'review_body', 'verified_purchase']]\n",
        "df = df.dropna()\n",
        "df = df[df['verified_purchase'] == 'Y']\n",
        "df['label'] = df['star_rating'].apply(lambda x: 1 if x > 3 else 0)\n",
        "df = df[['review_body', 'label']]\n",
        "df = df.sample(frac=1, random_state=42)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_size = int(0.8 * len(df))\n",
        "train_df = df[:train_size]\n",
        "valid_df = df[train_size:]\n",
        "\n",
        "# Vectorize the text data\n",
        "max_features = 10000\n",
        "sequence_length = 100\n",
        "vectorizer = TextVectorization(max_tokens=max_features, output_mode='int', output_sequence_length=sequence_length)\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(train_df['review_body'].values).batch(128)\n",
        "vectorizer.adapt(text_ds)\n",
        "\n",
        "# Create TensorFlow Datasets\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((vectorizer(train_df['review_body'].values), train_df['label'].values)).batch(32)\n",
        "valid_ds = tf.data.Dataset.from_tensor_slices((vectorizer(valid_df['review_body'].values), valid_df['label'].values)).batch(32)\n",
        "\n",
        "# Define your Keras model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(max_features + 1, 32),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adamax(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_ds, epochs=20, validation_data=valid_ds)\n",
        "\n",
        "# Evaluate the model using the validation data\n",
        "valid_loss, valid_acc = model.evaluate(valid_ds)\n",
        "print(f'Validation accuracy: {valid_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ain7NcHgZoew",
        "outputId": "410b8c58-ea34-483c-abad-7d711460e5d1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-5f7e56cb1e50>:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df = pd.read_csv(url, delimiter='\\t', compression='gzip', error_bad_lines=False)\n",
            "Skipping line 8704: expected 15 fields, saw 22\n",
            "Skipping line 16933: expected 15 fields, saw 22\n",
            "Skipping line 23726: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 85637: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 132136: expected 15 fields, saw 22\n",
            "Skipping line 158070: expected 15 fields, saw 22\n",
            "Skipping line 166007: expected 15 fields, saw 22\n",
            "Skipping line 171877: expected 15 fields, saw 22\n",
            "Skipping line 177756: expected 15 fields, saw 22\n",
            "Skipping line 181773: expected 15 fields, saw 22\n",
            "Skipping line 191085: expected 15 fields, saw 22\n",
            "Skipping line 196273: expected 15 fields, saw 22\n",
            "Skipping line 196331: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 197000: expected 15 fields, saw 22\n",
            "Skipping line 197011: expected 15 fields, saw 22\n",
            "Skipping line 197432: expected 15 fields, saw 22\n",
            "Skipping line 208016: expected 15 fields, saw 22\n",
            "Skipping line 214110: expected 15 fields, saw 22\n",
            "Skipping line 244328: expected 15 fields, saw 22\n",
            "Skipping line 248519: expected 15 fields, saw 22\n",
            "Skipping line 254936: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 272057: expected 15 fields, saw 22\n",
            "Skipping line 293214: expected 15 fields, saw 22\n",
            "Skipping line 310507: expected 15 fields, saw 22\n",
            "Skipping line 312306: expected 15 fields, saw 22\n",
            "Skipping line 316296: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 336028: expected 15 fields, saw 22\n",
            "Skipping line 344885: expected 15 fields, saw 22\n",
            "Skipping line 352551: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 408773: expected 15 fields, saw 22\n",
            "Skipping line 434535: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 581593: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 652409: expected 15 fields, saw 22\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            " 1965/20767 [=>............................] - ETA: 9:10 - loss: 0.5144 - accuracy: 0.7825"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-5f7e56cb1e50>\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# Evaluate the model using the validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare with Naive Bayes and Logistic Regression classifiers\n",
        "print(f\"Naive Bayes test accuracy: {accuracy_score(y_test, nb_pred_test):.2f}\")\n",
        "print(f\"Naive Bayes validation accuracy: {accuracy_score(y_val, nb_pred_val):.2f}\")\n",
        "print(f\"Logistic Regression test accuracy: {accuracy_score(y_test, lr_pred_test):.2f}\")\n",
        "print(f\"Logistic Regression validation accuracy: {accuracy_score(y_val, lr_pred_val):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0In0KR-f1FtV",
        "outputId": "dd83d751-df69-479e-aecf-6ef4000105a9"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes test accuracy: 0.88\n",
            "Naive Bayes validation accuracy: 0.88\n",
            "Logistic Regression test accuracy: 0.90\n",
            "Logistic Regression validation accuracy: 0.90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part C(optional) using transfer learning Bert "
      ],
      "metadata": {
        "id": "Ui3dUIZqWmpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzQqeiSn-H7J",
        "outputId": "36c0186a-f2bb-4b9f-a0e0-2c12a1ff2c19"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer\n",
        "from transformers import TFBertModel\n",
        "\n",
        "\n",
        "\n",
        "# Keep only the 'review_body' and 'star_rating' columns\n",
        "df = df[['review_body', 'star_rating']]\n",
        "\n",
        "# Drop any rows with missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Convert the ratings to binary labels\n",
        "df['label'] = df['star_rating'].apply(lambda x: 1 if x > 3 else 0)\n",
        "\n",
        "# Initialize the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the input text and convert to input IDs\n",
        "def tokenize_text(text):\n",
        "    return np.array(tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=128))\n",
        "\n",
        "df['input_ids'] = df['review_body'].apply(tokenize_text)\n",
        "\n",
        "# Pad the input IDs to the maximum sequence length\n",
        "def pad_sequence(sequence):\n",
        "    sequence = sequence[:128]\n",
        "    sequence += [0] * (128 - len(sequence))\n",
        "    return sequence\n",
        "\n",
        "df['input_ids'] = df['input_ids'].apply(pad_sequence)\n",
        "\n",
        "# Convert the data to TensorFlow Datasets\n",
        "def map_example_to_dict(input_ids, label):\n",
        "    return {'input_ids': input_ids, 'attention_mask': tf.ones_like(input_ids), 'token_type_ids': tf.zeros_like(input_ids)}, label\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((df['input_ids'].values, df['label'].values))\n",
        "dataset = dataset.map(map_example_to_dict).batch(32)\n",
        "\n",
        "# Load the BERT model\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Freeze the BERT model layers\n",
        "bert_model.trainable = False\n",
        "\n",
        "# Define your Keras model\n",
        "inputs = tf.keras.Input(shape=(128,), dtype=tf.int32)\n",
        "bert_output = bert_model(inputs)\n",
        "dense1 = tf.keras.layers.Dense(256, activation='relu')(bert_output.pooler_output)\n",
        "dense2 = tf.keras.layers.Dense(128, activation='relu')(dense1)\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(dense2)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(dataset, epochs=10)\n",
        "\n",
        "# Evaluate the model\n",
        "model.evaluate(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "OSrJQYSEYv5b",
        "outputId": "2932ccaa-8352-4c6b-83a6-60da6ebffe55"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-7e1d49aeea52>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review_body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Pad the input IDs to the maximum sequence length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4769\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4770\u001b[0m         \"\"\"\n\u001b[0;32m-> 4771\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4773\u001b[0m     def _reduce(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;31m# self.f is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1175\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-7e1d49aeea52>\u001b[0m in \u001b[0;36mtokenize_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Tokenize the input text and convert to input IDs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review_body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m                 method).\n\u001b[1;32m   2331\u001b[0m         \"\"\"\n\u001b[0;32m-> 2332\u001b[0;31m         encoded_inputs = self.encode_plus(\n\u001b[0m\u001b[1;32m   2333\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2738\u001b[0m         )\n\u001b[1;32m   2739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2740\u001b[0;31m         return self._encode_plus(\n\u001b[0m\u001b[1;32m   2741\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2742\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m             )\n\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}